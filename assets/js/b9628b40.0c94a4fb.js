"use strict";(self.webpackChunkjorel_docs=self.webpackChunkjorel_docs||[]).push([[690],{7224:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>p,frontMatter:()=>t,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"basic-usage/advanced-configuration","title":"Advanced Configuration","description":"JorEl provides advanced configuration options for fine-tuning LLM behavior, including model-specific parameters and defaults. This guide covers configuration options beyond the basics.","source":"@site/docs/basic-usage/advanced-configuration.md","sourceDirName":"basic-usage","slug":"/basic-usage/advanced-configuration","permalink":"/jorel/docs/basic-usage/advanced-configuration","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9},"sidebar":"learn","previous":{"title":"Cancellation Support","permalink":"/jorel/docs/basic-usage/cancellation"},"next":{"title":"Examples","permalink":"/jorel/docs/examples"}}');var r=s(4848),o=s(8453);const t={sidebar_position:9},l="Advanced Configuration",d={},a=[{value:"Model-Specific Parameters",id:"model-specific-parameters",level:2},{value:"Reasoning Effort",id:"reasoning-effort",level:3},{value:"Verbosity",id:"verbosity",level:3},{value:"Max Tokens",id:"max-tokens",level:3},{value:"Model-Specific Defaults",id:"model-specific-defaults",level:2},{value:"Setting Defaults During Registration",id:"setting-defaults-during-registration",level:3},{value:"Configuration Precedence",id:"configuration-precedence",level:2},{value:"Temperature Handling",id:"temperature-handling",level:2},{value:"Stream Buffering Configuration",id:"stream-buffering-configuration",level:2},{value:"Model Override System",id:"model-override-system",level:2},{value:"Combining Advanced Options",id:"combining-advanced-options",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Provider-Specific Behavior",id:"provider-specific-behavior",level:2},{value:"Examples",id:"examples",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"advanced-configuration",children:"Advanced Configuration"})}),"\n",(0,r.jsx)(n.p,{children:"JorEl provides advanced configuration options for fine-tuning LLM behavior, including model-specific parameters and defaults. This guide covers configuration options beyond the basics."}),"\n",(0,r.jsx)(n.h2,{id:"model-specific-parameters",children:"Model-Specific Parameters"}),"\n",(0,r.jsx)(n.h3,{id:"reasoning-effort",children:"Reasoning Effort"}),"\n",(0,r.jsx)(n.p,{children:"For reasoning models (like OpenAI's o1 and o3 series), you can control the computational effort:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'import { JorEl } from \'jorel\';\n\nconst jorEl = new JorEl({ openAI: true });\n\nconst response = await jorEl.text(\n  "Solve this complex logic puzzle...",\n  {\n    model: "o3-mini",\n    reasoningEffort: "high" // Options: "minimal" | "low" | "medium" | "high"\n  }\n);\n'})}),"\n",(0,r.jsx)(n.p,{children:"Reasoning effort affects:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computational time"}),": Higher effort takes longer"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Token usage"}),": Higher effort uses more tokens"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Solution quality"}),": Higher effort may find better solutions"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Supported values:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:'"minimal"'})," - Fastest, lowest cost"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:'"low"'})," - Balanced for simple problems"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:'"medium"'})," - Default for most problems (if supported)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:'"high"'})," - Maximum effort for complex problems"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Provider support:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"OpenAI: Supported on o1 and o3 models"}),"\n",(0,r.jsx)(n.li,{children:"Other providers: Ignored (no error)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"verbosity",children:"Verbosity"}),"\n",(0,r.jsx)(n.p,{children:"Control the detail level of model responses (currently OpenAI-specific):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'const response = await jorEl.text(\n  "Explain quantum computing",\n  {\n    model: "gpt-4o",\n    verbosity: "low" // Options: "low" | "medium" | "high"\n  }\n);\n'})}),"\n",(0,r.jsx)(n.p,{children:"Verbosity affects:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Response length"}),": Higher verbosity = longer responses"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Detail level"}),": More explanation and context"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Token usage"}),": Higher verbosity uses more tokens"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Supported values:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:'"low"'})," - Concise, to the point"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:'"medium"'})," - Balanced detail level"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:'"high"'})," - Detailed explanations"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Provider support:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"OpenAI: Supported (may vary by model)"}),"\n",(0,r.jsx)(n.li,{children:"Other providers: Ignored (no error)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"max-tokens",children:"Max Tokens"}),"\n",(0,r.jsx)(n.p,{children:"Limit the maximum number of tokens in the response:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'const response = await jorEl.text(\n  "Write an essay",\n  {\n    maxTokens: 500 // Limit to 500 tokens\n  }\n);\n'})}),"\n",(0,r.jsx)(n.p,{children:"This is useful for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Controlling costs"}),"\n",(0,r.jsx)(n.li,{children:"Ensuring responses fit in UI elements"}),"\n",(0,r.jsx)(n.li,{children:"Preventing overly long responses"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Provider support:"})," All providers"]}),"\n",(0,r.jsx)(n.h2,{id:"model-specific-defaults",children:"Model-Specific Defaults"}),"\n",(0,r.jsx)(n.p,{children:"You can configure default settings that apply whenever a specific model is used:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'const jorEl = new JorEl({ openAI: true });\n\n// Set defaults for a specific model\njorEl.models.setModelSpecificDefaults("o3-mini", {\n  reasoningEffort: "medium",\n  temperature: null // Explicitly disable temperature\n});\n\njorEl.models.setModelSpecificDefaults("gpt-4o", {\n  temperature: 0.7,\n  verbosity: "medium"\n});\n\n// Now all requests with o3-mini use medium reasoning effort\nconst response = await jorEl.text(\n  "Solve this puzzle",\n  { model: "o3-mini" }\n  // reasoningEffort: "medium" is automatically applied\n);\n'})}),"\n",(0,r.jsx)(n.h3,{id:"setting-defaults-during-registration",children:"Setting Defaults During Registration"}),"\n",(0,r.jsx)(n.p,{children:"You can also set defaults when registering a model:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'jorEl.models.register({\n  model: "o3-mini",\n  provider: "openai",\n  defaults: {\n    reasoningEffort: "high",\n    temperature: null\n  }\n});\n\n// Or using provider-specific methods\njorEl.providers.openAi.addModel("o3-mini", false, {\n  reasoningEffort: "high",\n  temperature: null\n});\n'})}),"\n",(0,r.jsx)(n.h2,{id:"configuration-precedence",children:"Configuration Precedence"}),"\n",(0,r.jsx)(n.p,{children:"JorEl applies configuration in this order (highest to lowest priority):"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Request-level configuration"})," - Parameters passed directly to ",(0,r.jsx)(n.code,{children:"text()"}),", ",(0,r.jsx)(n.code,{children:"json()"}),", or ",(0,r.jsx)(n.code,{children:"stream()"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model-specific defaults"})," - Defaults set for the specific model being used"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Instance-level configuration"})," - Defaults set on the JorEl instance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Model overrides"})," - Automatic adjustments for model limitations"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Example:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'const jorEl = new JorEl({\n  openAI: true,\n  temperature: 0 // Instance default\n});\n\njorEl.models.setModelSpecificDefaults("gpt-4o", {\n  temperature: 0.7, // Model default\n  verbosity: "low"\n});\n\n// This request uses temperature: 1 (request-level wins)\nconst response1 = await jorEl.text("Hello", {\n  model: "gpt-4o",\n  temperature: 1\n});\n\n// This request uses temperature: 0.7 (model default wins)\nconst response2 = await jorEl.text("Hello", {\n  model: "gpt-4o"\n});\n\n// This request uses temperature: 0 (instance default wins)\nconst response3 = await jorEl.text("Hello", {\n  model: "gpt-4o-mini" // No model-specific defaults\n});\n'})}),"\n",(0,r.jsx)(n.h2,{id:"temperature-handling",children:"Temperature Handling"}),"\n",(0,r.jsxs)(n.p,{children:["Temperature can be explicitly unset using ",(0,r.jsx)(n.code,{children:"null"})," , which is useful for models that don't support it:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'// Explicitly disable temperature for a request\nconst response = await jorEl.text(\n  "Solve this problem",\n  {\n    model: "o3-mini",\n    temperature: null // Don\'t send temperature parameter\n  }\n);\n\n// Set as default for a model\njorEl.models.setModelSpecificDefaults("o3-mini", {\n  temperature: null\n});\n'})}),"\n",(0,r.jsx)(n.p,{children:"JorEl automatically handles temperature for models that don't support it (like o1 and o3 series), but you can also be explicit."}),"\n",(0,r.jsx)(n.h2,{id:"stream-buffering-configuration",children:"Stream Buffering Configuration"}),"\n",(0,r.jsx)(n.p,{children:"Control how streaming chunks are emitted:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'const stream = jorEl.stream("Generate a story", {\n  streamBuffer: {\n    bufferTimeMs: 200, // Wait 200ms before emitting buffered content\n    disabled: false     // Set to true to disable buffering\n  }\n});\n'})}),"\n",(0,r.jsxs)(n.p,{children:["See ",(0,r.jsx)(n.a,{href:"/jorel/docs/basic-usage/generating-responses#stream-buffering",children:"Generating Responses"})," for more details."]}),"\n",(0,r.jsx)(n.h2,{id:"model-override-system",children:"Model Override System"}),"\n",(0,r.jsx)(n.p,{children:"JorEl automatically adjusts for model limitations:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'// When using a model that doesn\'t support system messages\nconst response = await jorEl.text("Hello", {\n  model: "o3-mini",\n  systemMessage: "Be helpful" // Automatically filtered out\n});\n// JorEl logs: "System messages are not supported for o3-mini and will be ignored"\n\n// When using a model that doesn\'t support temperature\nconst response2 = await jorEl.text("Hello", {\n  model: "o3-mini",\n  temperature: 0.7 // Automatically removed\n});\n// JorEl logs: "Temperature is not supported for o3-mini and will be ignored"\n'})}),"\n",(0,r.jsx)(n.p,{children:"This happens automatically - no action needed."}),"\n",(0,r.jsx)(n.h2,{id:"combining-advanced-options",children:"Combining Advanced Options"}),"\n",(0,r.jsx)(n.p,{children:"You can combine multiple advanced options:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:'const response = await jorEl.text(\n  "Solve this complex problem: ...",\n  {\n    model: "o3-mini",\n    reasoningEffort: "high",\n    maxTokens: 1000,\n    temperature: null,\n    abortSignal: controller.signal,\n    streamBuffer: { bufferTimeMs: 100 }\n  }\n);\n'})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Set model-specific defaults"})," for models you use frequently"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use reasoningEffort judiciously"})," - higher values significantly increase cost"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Always set maxTokens"})," in production to control costs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use null temperature explicitly"})," for reasoning models to avoid confusion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Combine with cancellation"})," for user-facing applications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Monitor token usage"})," when using high verbosity or reasoning effort"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"provider-specific-behavior",children:"Provider-Specific Behavior"}),"\n",(0,r.jsx)(n.p,{children:"Different providers handle advanced parameters differently:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"OpenAI"}),(0,r.jsx)(n.th,{children:"Anthropic"}),(0,r.jsx)(n.th,{children:"Google"}),(0,r.jsx)(n.th,{children:"Others"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"reasoningEffort"})}),(0,r.jsx)(n.td,{children:"\u2705 Certain models"}),(0,r.jsx)(n.td,{children:"\u274c Ignored"}),(0,r.jsx)(n.td,{children:"\u274c Ignored"}),(0,r.jsx)(n.td,{children:"\u274c Ignored"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"verbosity"})}),(0,r.jsx)(n.td,{children:"\u2705 Supported"}),(0,r.jsx)(n.td,{children:"\u274c Ignored"}),(0,r.jsx)(n.td,{children:"\u274c Ignored"}),(0,r.jsx)(n.td,{children:"\u274c Ignored"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"maxTokens"})}),(0,r.jsx)(n.td,{children:"\u2705 Supported"}),(0,r.jsx)(n.td,{children:"\u2705 Supported"}),(0,r.jsx)(n.td,{children:"\u2705 Supported"}),(0,r.jsx)(n.td,{children:"\u2705 Supported"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"temperature"})}),(0,r.jsx)(n.td,{children:"\u2705 Supported"}),(0,r.jsx)(n.td,{children:"\u2705 Supported"}),(0,r.jsx)(n.td,{children:"\u2705 Supported"}),(0,r.jsx)(n.td,{children:"\u2705 Most support"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:"Parameters are safely ignored when not supported (no errors thrown)."}),"\n",(0,r.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,r.jsx)(n.p,{children:"Working examples are available in the repository:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"examples/standard-use/open-ai/text-reasoning.ts"})," - Reasoning effort examples"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"examples/standard-use/open-ai/text-with-parameters.ts"})," - Advanced parameters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"examples/standard-use/open-ai/stream-reasoning.ts"})," - Streaming with reasoning"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>l});var i=s(6540);const r={},o=i.createContext(r);function t(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);