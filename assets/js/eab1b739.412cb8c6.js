"use strict";(self.webpackChunkjorel_docs=self.webpackChunkjorel_docs||[]).push([[23],{4229:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"basic-usage/token-tracking","title":"Token Tracking","description":"JorEl automatically tracks token usage across all generations, including complex scenarios where multiple API calls are made due to tool usage. This helps you monitor costs and optimize your LLM usage.","source":"@site/docs/basic-usage/token-tracking.md","sourceDirName":"basic-usage","slug":"/basic-usage/token-tracking","permalink":"/jorel/docs/basic-usage/token-tracking","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"learn","previous":{"title":"Using Tools","permalink":"/jorel/docs/basic-usage/tools"},"next":{"title":"Cancellation Support","permalink":"/jorel/docs/basic-usage/cancellation"}}');var o=t(4848),i=t(8453);const r={sidebar_position:7},a="Token Tracking",l={},c=[{value:"Basic Token Tracking",id:"basic-token-tracking",level:2},{value:"Multi-Generation Token Tracking",id:"multi-generation-token-tracking",level:2},{value:"Generation Details",id:"generation-details",level:2},{value:"Streaming with Token Tracking",id:"streaming-with-token-tracking",level:2},{value:"Token Tracking with Agents",id:"token-tracking-with-agents",level:2},{value:"Provider Support",id:"provider-support",level:2},{value:"Cost Calculation",id:"cost-calculation",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Examples",id:"examples",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"token-tracking",children:"Token Tracking"})}),"\n",(0,o.jsx)(n.p,{children:"JorEl automatically tracks token usage across all generations, including complex scenarios where multiple API calls are made due to tool usage. This helps you monitor costs and optimize your LLM usage."}),"\n",(0,o.jsx)(n.h2,{id:"basic-token-tracking",children:"Basic Token Tracking"}),"\n",(0,o.jsxs)(n.p,{children:["When you request metadata from ",(0,o.jsx)(n.code,{children:"text"})," , ",(0,o.jsx)(n.code,{children:"json"})," , or ",(0,o.jsx)(n.code,{children:"streamWithMeta"})," methods, JorEl includes token usage information:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"import { JorEl } from 'jorel';\n\nconst jorEl = new JorEl({ openAI: true });\n\nconst { response, meta } = await jorEl.text(\n  \"What is the capital of France?\",\n  { model: \"gpt-4o-mini\" },\n  true // Request metadata\n);\n\nconsole.log(meta);\n// {\n//   model: 'gpt-4o-mini',\n//   provider: 'openai',\n//   temperature: 0,\n//   durationMs: 730,\n//   inputTokens: 26,\n//   outputTokens: 16\n// }\n"})}),"\n",(0,o.jsx)(n.h2,{id:"multi-generation-token-tracking",children:"Multi-Generation Token Tracking"}),"\n",(0,o.jsx)(n.p,{children:"When using tools, JorEl often makes multiple API calls to the LLM:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Initial generation: Model decides to call tools"}),"\n",(0,o.jsx)(n.li,{children:"Tool execution: Your functions are called"}),"\n",(0,o.jsx)(n.li,{children:"Subsequent generation(s): Model synthesizes the final answer with tool results"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Starting in v1.0.0, JorEl accurately tracks cumulative token usage across all these generations:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:'import { JorEl } from \'jorel\';\nimport { z } from \'zod\';\n\nconst jorEl = new JorEl({ openAI: true });\n\nconst { response, meta } = await jorEl.text(\n  "What\'s the weather in Paris and what\'s the stock price of AAPL?",\n  {\n    tools: [{\n      name: "get_weather",\n      description: "Get weather for a city",\n      executor: async ({ city }) => ({ temperature: 22, conditions: "sunny" }),\n      params: z.object({ city: z.string() })\n    }, {\n      name: "get_stock_price",\n      description: "Get stock price",\n      executor: async ({ ticker }) => ({ price: 150.25 }),\n      params: z.object({ ticker: z.string() })\n    }]\n  },\n  true\n);\n\nconsole.log(meta.inputTokens);   // Total input tokens across all generations\nconsole.log(meta.outputTokens);  // Total output tokens across all generations\nconsole.log(meta.durationMs);    // Total duration\n'})}),"\n",(0,o.jsx)(n.h2,{id:"generation-details",children:"Generation Details"}),"\n",(0,o.jsxs)(n.p,{children:["When multiple generations occur, the metadata includes a ",(0,o.jsx)(n.code,{children:"generations"})," array with details about each individual API call:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:'const { response, meta } = await jorEl.text(\n  "What\'s the weather in Sydney?",\n  {\n    tools: [{\n      name: "get_weather",\n      executor: getWeather,\n      params: weatherSchema\n    }]\n  },\n  true\n);\n\nif (meta.generations && meta.generations.length > 1) {\n  console.log(`Made ${meta.generations.length} API calls`);\n  \n  meta.generations.forEach((gen, index) => {\n    console.log(`Generation ${index + 1}:`);\n    console.log(`  Type: ${gen.hadToolCalls ? "Tool Call Request" : "Final Response"}`);\n    console.log(`  Model: ${gen.model}`);\n    console.log(`  Input Tokens: ${gen.inputTokens}`);\n    console.log(`  Output Tokens: ${gen.outputTokens}`);\n    console.log(`  Duration: ${gen.durationMs}ms`);\n  });\n}\n'})}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"generations"})," array is only included when multiple generations occur, keeping the response lightweight for simple requests."]}),"\n",(0,o.jsx)(n.h2,{id:"streaming-with-token-tracking",children:"Streaming with Token Tracking"}),"\n",(0,o.jsxs)(n.p,{children:["Token tracking also works with streaming responses. Use ",(0,o.jsx)(n.code,{children:"streamWithMeta"})," to access token information:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:'const stream = jorEl.streamWithMeta(\n  "What\'s the weather in New York?",\n  { tools: [weatherTool] }\n);\n\nfor await (const chunk of stream) {\n  if (chunk.type === "chunk") {\n    process.stdout.write(chunk.content);\n  } else if (chunk.type === "response") {\n    console.log("\\nToken usage:", {\n      input: chunk.meta.inputTokens,\n      output: chunk.meta.outputTokens,\n      duration: chunk.meta.durationMs\n    });\n  }\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"token-tracking-with-agents",children:"Token Tracking with Agents"}),"\n",(0,o.jsx)(n.p,{children:"When using JorEl's agent system, token usage is automatically tracked across all agent interactions:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"const task = await jorEl.team.createTask(\"Research topic X\");\n\nconst result = await jorEl.team.executeTask(task, {\n  limits: { maxIterations: 10 }\n});\n\nconst { tokens } = result.eventsWithStatistics;\n\nconsole.log(tokens);\n// {\n//   'gpt-4o-mini': { input: 1250, output: 380 },\n//   'gpt-4': { input: 450, output: 120 }\n// }\n"})}),"\n",(0,o.jsx)(n.p,{children:"Token usage is grouped by model, making it easy to calculate costs across different models."}),"\n",(0,o.jsx)(n.h2,{id:"provider-support",children:"Provider Support"}),"\n",(0,o.jsx)(n.p,{children:"Token tracking is available for all providers that report token usage:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"OpenAI"}),": Full support"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Anthropic"}),": Full support"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Google Vertex AI"}),": Full support"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Google Generative AI"}),": Full support"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Groq"}),": Full support"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Grok"}),": Full support"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Mistral"}),": Full support"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Ollama"}),": Limited (depends on model)"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["When a provider doesn't report token counts, the fields will be ",(0,o.jsx)(n.code,{children:"undefined"})," ."]}),"\n",(0,o.jsx)(n.h2,{id:"cost-calculation",children:"Cost Calculation"}),"\n",(0,o.jsx)(n.p,{children:"You can use token counts to estimate costs:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-typescript",children:"const COST_PER_1K_INPUT = 0.00015;  // GPT-4o-mini\nconst COST_PER_1K_OUTPUT = 0.0006;\n\nconst { meta } = await jorEl.text(prompt, {}, true);\n\nconst cost = \n  (meta.inputTokens / 1000 * COST_PER_1K_INPUT) +\n  (meta.outputTokens / 1000 * COST_PER_1K_OUTPUT);\n\nconsole.log(`Request cost: $${cost.toFixed(6)}`);\n"})}),"\n",(0,o.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Always request metadata"})," when you need to track usage or costs"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Check the generations array"})," to understand multi-step tool usage patterns"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Monitor token usage"})," in production to optimize prompts and tools"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Use token tracking with agents"})," to understand complex task costs"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Consider caching"})," expensive tool results to reduce token usage"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,o.jsx)(n.p,{children:"You can find complete working examples in the repository:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"examples/standard-use/open-ai/text-with-token-tracking.ts"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"examples/standard-use/open-ai/stream-with-token-tracking.ts"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const o={},i=s.createContext(o);function r(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);